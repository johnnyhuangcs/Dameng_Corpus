

# 基本概念与原理

本章介绍 DM MPP 的一些基础概念与基本原理，虽然读者跳过这一章继续阅读后续章节也能进行 DM MPP 的相关操作，但是本章的阅读有利于读者进一步理解 DM MPP 的工作原理，对于后续对 MPP 系统的规划与管理能起到一定帮助。

## 3.1 基本概念

### 3.1.1 执行节点 EP

DM MPP 系统中每一个运行的 DM 数据库服务器实例称为一个执行节点 EP，基于数据守护的 MPP 环境内的备库除外。

### 3.1.2 主、从 EP

DM MPP 采用完全对等无共享架构，对整个系统来说，每个 EP 作用都是一样的，用户可以连接到其中的任何一个进行操作。而对每个用户会话来说，EP 具有主从之分。用户会话实际连接的那个 EP 对该用户会话来说称为主 EP，其余的 EP 都称为从 EP。

### 3.1.3 数据分布

DM MPP 系统中的数据分布在各 EP 中，支持表数据的哈希分布、随机分布、复制分布、范围分布、LIST 分布类型，用户可根据应用的实际情况为表数据选择合适的分布类型。

  * **哈希分布**



哈希分布按照表定义中指定的一列或多列对行数据计算一个哈希值，再根据哈希值和哈希映射表，将该行数据分布到映射的节点上。

当表的连接查询中使用的连接键为哈希分布列时，MPP 下的查询计划会进行优化，比如可能减少计划中通讯操作符个数、使用索引、对分组计划优化等，减少数据在节点间的分发，提高查询效率。

使用哈希分布时，节点间的数据是否均衡，取决于设置的哈希分布列以及表中的数据情况。当节点个数变动时，各个节点的数据需要按照新的哈希映射表重新进行分发。

  * **随机分布**



随机分布表不存在分布列，插入表数据时会按照一定的随机算法，将数据随机均衡分布到各个节点。

随机分布的优点是数据和节点间不存在映射关系。节点个数变动后，如果没有节点数据均衡的要求，可以不用对节点现有的数据进行变动。

一般来说，随机分布对于复杂查询及存在较多的节点间数据分发情况，性能不如哈希分布高。

  * **复制分布**



复制分布表在每个节点上的本地数据都是一份完整的拷贝，查询该表数据时在任意节点上都能单独完成，不需要从其他节点获取数据。

复制分布一般用于数据量不是很大的表。

  * **范围分布**



范围分布按照表定义中指定的一个或多个列的列值范围分布项，决定将一行数据存储到 MPP 的哪个相应 EP 上。

  * **LIST 分布**



LIST 分布通过指定表中的一个或多个列的离散值集，来确定将一行数据存储到 MPP 的哪个相应 EP 上。此分布用于表中列值可列举的情况。

> **建议**
>
> DM
> MPP同时支持数据分布与分区表，实现了“数据分布后再分区”。在数据分布到各节点的基础上，再在单个节点上将数据再次分区，可进一步提高查询性能。分布的类型和分区的类型可以混合搭配，比如建立哈希分布表的范围水平分区表。



### 3.1.4 MAL 系统

MAL 系统是 DM 数据库实例间的高速通信系统，是基于 TCP 协议实现的一种内部通信机制，具有可靠、灵活、高效的特性。DM 通过 MAL 系统实现实例间的消息通讯。

### 3.1.5 全局连接与本地连接

MPP 系统中数据分布在各个 EP 中，用户只需要登录到某个 EP，系统自动建立这个 EP 与其余 EP 的连接，因此用户建立的实际上是与整个 MPP 系统的全局连接，用户对数据库的操作通过全局连接在 MPP 系统的所有 EP 进行。使用全局连接时，要求 MPP 系统的所有 EP 都正常提供服务，否则无法建立连接。

DM MPP 也提供本地连接。当使用本地连接时，用户登录到某个 EP 后，这个 EP 不再建立与其余 EP 的连接，用户的所有数据库操作仅在这个 EP 上进行。如 SELECT 语句以及 UPDATE 和 DELETE 语句中的 WHERE 条件中的子查询都仅仅查询本地 EP 的数据，而 INSERT 语句如果插入的数据根据分布定义应分布在其余 EP 时系统会报错。

通常在 MPP 系统正常运行时都使用全局连接，DM MPP 的快速装载使用到了本地连接，用户在某些时候如 MPP 系统中有 EP 故障时也可以使用本地连接。

DM 的各接口驱动程序都提供了连接属性用于设置全局连接（登录）或本地连接（登录），缺省都为全局连接。DM 交互式工具 DIsql 也提供了登录参数 MPP_TYPE 用来指定使用全局连接或本地连接，“GLOBAL”表示全局连接，“LOCAL”表示本地连接，默认为全局连接。

## 3.2 MPP 并行执行计划

在 DM 数据库中，SQL 语句经过一系列的处理最终生成一棵由不同操作符组成的计划树，DM 执行器以自底向上的顺序执行计划树，数据也按自底向上的顺序在计划树中流动并经过各操作符的处理，最终在计划树的根节点生成执行结果。

在 DM MPP 环境中各 EP 执行的是并行计划，并行计划是在单节点执行计划的基础上，按照一定规则于适当的位置插入 MPP 通讯操作符而生成的。

本节介绍 DM MPP 并行执行计划的原理与处理过程。

### 3.2.1 并行计划相关操作符

MPP 并行执行计划是在单节点执行计划的基础上增加 MPP 通讯操作符生成的，DM MPP 相关通讯操作符有 5 个，如下表所示。

表3.1 MPP并行执行计划通讯操作符

|**操作符名称**|**功能**|
|-----|-----|
|MPP GATHER （MGAT）|主 EP 收集所有节点数据从 EP 将数据发送到主 EP|
|MPP COLLECT （MCLCT）|在 MGAT 的基础上，增加主从 EP 执行同步功能，避免数据在主 EP 上堆积。一个计划树中一般只会在较上层出现一个 MCLCT，但可能有多个 MGAT|
|MPP DISTRIBUTE （MDIS）|各 EP 节点间相互分发数据，按照分发列计算行数据的目标节点并发送过去，目标节点负责接收|
|MPP BROADCAST （MBRO）|功能类似 MGAT，收集数据到主 EP，该操作符带有聚集函数运算功能，仅和 FAGR 配合使用|
|MPP SCATTER （MSCT）|主 EP 发送完整数据到所有从 EP，保证每个节点数据都完整，一般和 MGAT 配合使用|


### 3.2.2 并行计划的生成

在 DM 数据库中，计划树执行时，数据总是自底向上流动的，若是在 MPP 环境中，由于每个 EP 都只保存表的部分数据（复制分布除外），计划树叶子节点的数据都是不完整的，因此叶子节点向上传送的只是本 EP 部分的数据。要获得完整的正确执行结果，上层节点必须知道此时下层节点传来的数据是否完整，从而决定是否需要添加 MPP 通讯操作符，并根据单节点操作符的功能和数据分布情况决定应该添加何种通讯操作符。关于单节点执行计划操作符的介绍可以参看《DM8 系统管理员手册》的附录。

下面通过几个简单的例子说明 MPP 并行计划生成的原理。

首先，在 MPP 环境中建两个表 T1 与 T2，都使用哈希分布。

```
CREATE TABLE T1(C1 INT, C2 INT) DISTRIBUTED BY HASH(C1);

CREATE TABLE T2(C3 INT, C4 INT) DISTRIBUTED BY HASH(C3);
```

  * 例 1 单表查询，查询表 T1 的全部数据。



使用 EXPLAIN 查看语句的执行计划：

```
EXPLAIN SELECT * FROM T1;
```

显示的并行执行计划如下：

```
1 ##NSET2: [0, 1, 16]

2	 ##MPP COLLECT: [0, 1, 16]; op_id(1) n_grp_by (0) n_cols(0) n_keys(0)

3		 ##PRJT2: [0, 1, 16]; exp_num(3), is_atom(FALSE)

4 			##CSCN2: [0, 1, 16]; INDEX33555445(T1)
```

可以看到，这条语句的并行执行计划只是在单节点执行计划中增加了一个 MPP COLLECT 通讯操作符，用于在从 EP 扫描和投影数据后，主 EP 收集所有 EP 的数据。

  * 例 2 连接查询，连接键不是分布列的情况。



使用 EXPLAIN 查看语句的执行计划：

```
EXPLAIN SELECT * FROM T1 INNER JOIN T2 ON T1.C2=T2.C4;
```

显示的并行执行计划如下：

```
1 ##NSET2: [0, 1, 16]

2 	##MPP COLLECT: [0, 1, 16]; op_id(3) n_grp_by (0) n_cols(0) n_keys(0)

3 		##PRJT2: [0, 1, 16]; exp_num(4), is_atom(FALSE)

4 			##HASH2 INNER JOIN: [0, 1, 16]; KEY_NUM(1);

5 				##MPP DISTRIBUTE: [0, 1, 8]; op_id(1) n_keys(1) n_grp(0) filter(FALSE ) rowid_flag(0)

6					 ##CSCN2: [0, 1, 8]; INDEX33555445(T1)

7 						##MPP DISTRIBUTE: [0, 1, 8]; op_id(2) n_keys(1) n_grp(0) filter(FALSE) rowid_flag(0)

8 					##CSCN2: [0, 1, 8]; INDEX33555446(T2)
```

在这个并行计划中，哈希连接操作符“HASH2 INNER JOIN”的左右孩子 CSCN2 都各增加了一个 MPP DISTRIBUTE 通讯操作符，用于将数据按照连接键分发到各 EP 进行连接操作，之后计划的上层增加 MPP COLLECT 通讯操作符将各 EP 的连接和投影结果收集到主 EP。

  * 例 3 集函数查询，查询表 T1 的行数。



使用 EXPLAIN 查看语句的执行计划：

```
EXPLAIN SELECT COUNT(*) FROM T1;
```

显示的并行执行计划如下：

```
1 ##NSET2: [0, 1, 0]

2 	##MPP COLLECT: [0, 1, 0]; op_id(3) n_grp_by (0) n_cols(0) n_keys(0)

3 		##PRJT2: [0, 1, 0]; exp_num(1), is_atom(FALSE)

4 			##MPP BROADCAST: [0, 1, 0]; op_id(1)

5 				##FAGR2: [0, 1, 0]; sfun_num(1),
```

在这个并行计划中，集函数操作符 FAGR2 的上层增加了 MPP BROADCAST 通讯操作符，使得主 EP 可以收集并合并各从 EP 的集函数操作符执行结果。在此计划中，上层的 MPP COLLECT 操作符只起同步执行的作用，并不收集从 EP 的数据。

  * 例 4 连接查询，执行计划中使用嵌套连接操作符的情况。



使用 EXPLAIN 查看语句的执行计划：

```
EXPLAIN SELECT * FROM T1 INNER JOIN T2 ON T1.C1<>T2.C4;
```

显示的并行执行计划如下：

```
1 ##NSET2: [49, 1, 16]

2 	##MPP COLLECT: [49, 1, 16]; op_id(4) n_grp_by (0) n_cols(0) n_keys(0)

3 		##PRJT2: [49, 1, 16]; exp_num(4), is_atom(FALSE)

4 			##SLCT2: [49, 1, 16]; T1.C1 <> T2.C4

5 				##NEST LOOP INNER JOIN2: [49, 1, 16];

6 					##NTTS2: [0, 1, 8]; for_mdis(TRUE)

7 						##MPP GATHER: [0, 1, 8]; op_id(1) n_grp_by (0) n_cols(0) n_keys(0)

8 						##CSCN2: [0, 1, 8]; INDEX33555445(T1)

9 					##NTTS2: [0, 1, 8]; for_mdis(TRUE)

10 						##MPP GATHER: [0, 1, 8]; op_id(2) n_grp_by (0) n_cols(0) n_keys(0)

11 						##CSCN2: [0, 1, 8]; INDEX33555446(T2)
```

由于嵌套连接操作符的特殊性，左表数据要和右表的全部数据都比较一遍，右表需要多次执行，因此在这个并行执行计划中增加了 MPP GATHER 通讯操作符将各从 EP 的数据完整收集到主 EP。计划上层的 MPP COLLECT 通讯操作符只起同步执行的作用，并不收集从 EP 的数据。

以上举了几个简单的例子说明 DM 是如何在单节点执行计划中增加 MPP 通讯操作符生成 MPP 并行执行计划的。DM 数据库的操作符众多，并行计划也复杂多变，这里不能一一介绍，用户可通过查看语句的并行执行计划体会 DM MPP 并行执行计划的生成规则。

### 3.2.3 数据分布与并行执行计划

在 MPP 环境中，建表时指定的数据分布类型决定了表数据的分布。DM MPP 支持的表分布类型包括哈希分布、随机分布、范围分布、复制分布、LIST 分布，数据插入或装载时系统会根据表的分布类型自动将数据发送到对应的 EP。

哈希分布、范围分布和 LIST 分布的共同特征是在创建表的时候，用户指定一个或多个列作为分布列，系统会针对每个插入的数据行计算这些对应列的值，确定将数据所属 EP。随机分布和复制分布则不需要指定分布列。

并行执行计划与数据分布密切相关，数据分布能够决定最终生成的并行计划。例如查询的数据经过预先判断发现正好都在一个 EP 上，服务器会做一定的优化，在最优的情况下，整个计划甚至不包含任何通讯操作符。优化的原则是尽量减少节点之间的通信交互。

因此，用户应根据应用中查询的实际需求来确定表的分布类型，进而得到较优的并行执行计划。

  * **场景一**



在某应用中，查询语句中包含大量连接查询，表数据分布较为均匀，应用对查询的效率要求较高。此时我们可以使用哈希分布，并且将常用连接列作为哈希分布列，这样能尽可能地减少 EP 间的数据传递，少占用网络带宽，减少网络延迟，充分发挥多节点并行执行的巨大优势。

  * **场景二**



在某应用中，查询以单表查询居多，连接查询较少，我们可以采用随机分布。随机分布使得海量数据能均匀分布，充分体现 MPP 的并行优势。

  * **场景三**



对于单表查询或出现在连接查询中数据量较小的表，可采用复制分布。复制分布的表在每个 EP 上都有一份完整的数据拷贝，使得在生成并行执行计划时能减少对应通讯操作符的使用，进一步优化并行计划。

### 3.2.4 并行计划执行流程

DM MPP 对于查询语句和插入/删除/修改语句的处理是不同的，因为插入/删除/修改语句涉及到数据的修改，必须在数据行所在 EP 进行修改操作，而查询语句只需要主 EP 收集查询结果即可。

#### 3.2.4.1 查询语句处理流程

DM MPP 对查询语句的处理按如下流程进行：

  1. 建立连接



用户连接到 MPP 系统内任意一个 EP 节点，则该 EP 为连接的主 EP，其余节点为从 EP;

  2. 生成执行计划



主 EP 解析查询语句，生成普通的查询计划后，根据数据分布情况在合适的位置插入合适的并行通讯操作符，生成最终的并行查询计划;

  3. 分发计划



主 EP 把执行计划分发给所有的从 EP;

  4. 执行计划



各从 EP 收到计划后，生成执行计划的运行环境，所有 EP 并行执行，执行时各 EP 通过通讯操作符分发必要的数据并协调执行进度;

  5. 生成结果集



主 EP 收集所有 EP 的查询结果（包括自身数据），生成结果集;

  6. 返回结果集



主 EP 将结果集返回给用户

#### 3.2.4.2 插入/修改/删除语句处理流程

DM MPP 对插入/修改/删除语句的处理按如下流程进行：

  1. 建立连接



用户连接到 MPP 系统内任意一个 EP 节点，则该 EP 为连接的主 EP，其余节点为从 EP；

2.生成执行计划

主 EP 解析语句，生成执行计划，其中包含的查询计划（即 WHERE 条件对应的计划）也是并行查询计划，另外还会生成一个对应的在从 EP 上执行的计划（MPLN）；

  3. 准备数据



主 EP 开始执行计划时首先把查询计划部分发布给所有的从 EP，并行执行查询，主 EP 收集查询结果；

  4. 定位节点



数据准备完成后，根据分布列和分布方式计算出需要修改的行数据所在的目标 EP，将 MPLN 以及操作所需数据发送到各对应的 EP。如果目标 EP 为本地则不发送，在本地直接完成操作；

  5. 执行修改操作



从 EP 收到 MPLN 计划和数据后生成执行环境，执行实际的修改操作；

  6. 返回执行结果



主 EP 等待所有的从 EP 执行完成后才会返回执行结果给客户端，只要其中有任一个 EP 执行失败，则已经执行的所有 EP 都会回滚，保证数据的一致性，并返回错误信息给客户端。

### 3.2.5 DDL 语句分发

与并行计划执行流程中介绍的 DML 语句并行执行计划处理流程不同，DM MPP 对 DDL 语句的处理采用语句分发方式。主 EP 直接将 DDL 语句发送给各从 EP，每个从 EP 各自执行该 DDL 语句。主 EP 等待所有的从 EP 执行完成后才返回消息给用户，只要其中有任一个 EP 执行失败，则已经执行的所有 EP 都会进行回滚，保证数据的一致性，并返回错误信息给客户端。

由于 DDL 语句分发，使得在 MPP 中登录任一 EP 执行的 DDL 操作都是全局的，包括数据库对象的建立、修改和删除等，也包括用户的建立、修改与删除。

## 3.3 相关配置文件

### 3.3.1 dm.ini MPP 相关配置项

dm.ini 是 DM 数据库实例的配置文件，通过配置该文件可以设置 DM 数据库服务器的各种功能和性能选项。dm.ini 中的配置项多达数百个，涉及 DM 数据库运行各个方面的设置，这里我们只介绍与 MPP 相关的几个配置项，读者若对其他配置项感兴趣可以参看《DM8 系统管理员手册》的相关章节。

表3.2 dm.ini MPP相关配置项

|**配置项**|**配置含义**|
|----|----|
|INSTANCE_NAME|数据库实例名（长度不超过 16 个字符）|
|PORT_NUM|DM 服务器监听通讯端口号，服务器配置此参数，取值范围 1024~65534，发起连接端的端口在 1024~65535 之间随机分配|
|MAL_INI|MAL 系统配置开关，0：表示不启用 MAL 系统，1：表示启用 MAL 系统，缺省为 0|
|MPP_INI|MPP 系统配置开关，0：表示不启用 MPP 系统，1：表示启用 MPP 系统，缺省为 0|
|MAX_EP_SITES|MPP 环境下 EP 节点的最大数量，取值范围 2~1024，缺省为 64|
|MPP_HASH_LR_RATE|MPP 下，对 HASH JOIN 节点，可以根据左右儿子 CARD 代价的比值，调整 HASH_JOIN 的左右儿子的 MOTION 添加，从而影响计划。如果 CARD 比值超过此值，则小数据量的一方全部收集到主 EP 来做。取值范围 1~4294967294，缺省为 10|
|MPP_OP_JUMP|MPP 系统中操作符的跳转开关，是否支持通讯操作符的跳转功能。1：支持；0：不支持。取值范围：0、1，缺省为 1|
|PHF_NTTS_OPT|MPP 系统中是否进行 NTTS 计划的优化，打开时可能减少计划中的 NTTS 操作符。1：支持；0：不支持。取值范围：0、1，缺省为 1|


另外，DM MPP 要求 MPP 系统中每个 EP 的建库参数及 dm.ini 中一些配置参数的配置值必须一致，在用户第一次全局登录 MPP 系统时会进行检查，如果设置不一致，则会报错。

这些配置项如下表所示。

表3.3 MPP中每个EP必须设置相同的配置项



### 3.3.2 dmmal.ini 配置项

dmmal.ini 是 MAL 系统的配置文件，此配置文件生效的前提是 dm.ini 中的参数 MAL_INI 置为 1。使用同一套 MAL 系统的所有实例，MAL 系统配置文件要严格保持一致。

dmmal.ini 的配置参数及其介绍见下表。

表3.4 dmmal.ini配置项

|**配置项**|**配置含义**|
|----|----|
|MAL_CHECK_INTERVAL|MAL 链路检测时间间隔，取值范围 0~1800，单位秒（s），缺省为 30s，配置为 0 表示不进行 MAL 链路检测。为了防止误判，DMRAC 集群中，建议设置配置值 >= DCR_GRP_NETCHK_TIME|
|MAL_CONN_FAIL_INTERVAL|判定实例之间 MAL 链路断开的时间，取值范围 2~1800，单位秒（s），缺省为 10s。仅当 MAL_CHECK_INTERVAL 不为 0 时有效|
|MAL_CHECK_TIMEOUT|单个实例的 MAL 网络最大延迟时间，单位毫秒（ms），取值范围 0~65535，缺省为 0，表示不进行 MAL 网络延迟时间检测。需要和 MAL_CHECK_IP 配合使用。仅当 MAL_CHECK_INTERVAL 不为 0 时有效|
|MAL_CHECK_IP|第三方确认机器的 IP，用于检测各个实例的 MAL 网络延迟时间。需要和 MAL_CHECK_TIMEOUT 配合使用。确认机器为 MAL 链路所有实例均可访问到的独立外网机器。 实例每隔 MAL_CHECK_INTERVAL 时间 ping 一次 MAL_CHECK_IP，若连接耗时超过 MAL_CHECK_TIMEOUT 指定时间，则该实例主动 HALT。 开启 MAL 网络延迟时间检测后，需要为 MAL 链路所有实例赋予 ping 权限，具体方法可以参考《DM8 数据共享集群》|
|MAL_LEAK_CHECK|是否打开 MAL 内存泄露检查，0：关闭，1：打开，缺省为 0|
|MAL_LOGIN_TIMEOUT|MPP/DBLINK 等实例间登录时的超时检测间隔，取值范围 3~1800，单位秒，缺省为 15s|
|MAL_BUF_SIZE|单个 MAL 缓存大小限制。当此 MAL 的缓存邮件超过此大小，则会将邮件存储到文件中。取值范围 0~500000，单位 MB，缺省为 100|
|MAL_SYS_BUF_SIZE|MAL 系统总内存大小限制。取值范围 0~500000，单位 MB，缺省为 0，表示 MAL 系统无总内存限制|
|MAL_VPOOL_SIZE|MAL 系统使用的内存初始化大小。取值范围 1~500000，单位 MB，缺省为 128，此值一般要设置的比 MAL_BUF_SIZE 大一些|
|MAL_COMPRESS_LEVEL|MAL 消息压缩等级，取值范围 0~10。缺省为 0，不进行压缩；1~9 表示采用 zip 算法，从 1 到 9 表示压缩速度依次递减，压缩率依次递增；10 表示采用 snappy 算法，压缩速度高于 zip 算法，压缩率相对低|
|MAL_TEMP_PATH|指定临时文件的目录。当邮件使用的内存超过 MAL_BUF_SIZE 或者 MAL_SYS_BUF_SIZE 时，将新产生的邮件保存到临时文件中。如果缺省，则新产生的邮件保存到 temp.dbf 文件中|
|[MAL_NAME]|MAL 名称，同一个配置文件中 MAL 名称需保持唯一性|
|MAL_INST_NAME|数据库实例名，与 dm.ini 的 INSTANCE_NAME 配置项保持一致，MAL 系统中数据库实例名要保持唯一|
|MAL_HOST|MAL IP 地址，使用 MAL_HOST + MAL_PORT 创建 MAL 链路|
|MAL_PORT|MAL 监听端口，用于数据守护、DSC、MPP 等环境中各节点实例之间 MAL 链路配置，监听端端口配置此参数，取值范围 1024~65534，发起连接端的端口在 1024~65535 之间随机分配|
|MAL_INST_HOST|MAL_INST_NAME 实例对外服务 IP 地址|
|MAL_INST_PORT|MAL_INST_NAME 实例服务器监听通讯端口号，服务器配置此参数，取值范围 1024~65534，发起连接端的端口在 1024~65535 之间随机分配  此参数的配置应与 dm.ini 中的 PORT_NUM 保持一致|
|MAL_DW_PORT|MAL_INST_NAME 实例守护进程的监听端口，其他守护进程或监视器使用 MAL_HOST + MAL_DW_PORT 创建与该实例守护进程的 TCP 连接，监听端配置此参数，取值范围 102465534，发起连接端的端口在 102465535 之间随机分配。缺省为 0，表示不配置端口号，无法监听|
|MAL_LINK_MAGIC|MAL 链路网段标识，取值范围 0~65535，缺省为 0。设置此参数时，同一网段内的节点都设置相同，不同网段内的节点设置的值必须不一样|


### 3.3.3 dmmpp.ctl

dmmpp.ctl 是 DM MPP 系统的控制文件，它是一个二进制文件，用户不能直接进行配置。用户需要首先配置 dmmpp.ini，然后利用 dmctlcvt 工具进行转换得到生成的 dmmpp.ctl 文件。MPP 系统中的每个 EP 都必须使用相同的 dmmpp.ctl 文件，进行文件拷贝即可。

dmmpp.ini 的配置参数及其介绍见下表。

表3.5 dmmpp.ini配置项

|**配置项**|**配置含义**|
|----|----|
|[SERVICE_NAME]|标识 MPP 系统中每个 EP 实例的选项名|
|MPP_SEQ_NO|实例在 mpp 系统内的序号，取值范围为 0~1023|
|MPP_INST_NAME|实例名|

